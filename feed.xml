<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://fluids.dev/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fluids.dev/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-17T10:36:42+00:00</updated><id>https://fluids.dev/feed.xml</id><title type="html">fluids, ml &amp;amp; more</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">SPHERIC 2024 Workshop</title><link href="https://fluids.dev/blog/2024/spheric/" rel="alternate" type="text/html" title="SPHERIC 2024 Workshop"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://fluids.dev/blog/2024/spheric</id><content type="html" xml:base="https://fluids.dev/blog/2024/spheric/"><![CDATA[<h1 id="18th-international-smoothed-particle-hydrodynamics-research-and-engineering-international-community-workshop">18th International Smoothed Particle Hydrodynamics Research and Engineering International Community Workshop</h1> <p>We have two papers in the proceedings this year:</p> <h2 id="physically-motivated-machine-learning-models-for-lagrangian-fluid-mechanics">Physically-Motivated Machine Learning Models for Lagrangian Fluid Mechanics</h2> <p>In this paper we talk about recent experiences and insights into machine learning for fluid mechanics, specifically Lagrangian ones. In this context we give an overview of some best practices and how we can bring CFD and ML knowledge together to build a bridge towards the future.</p> <p>This paper will be presented orally in Berlin and you can find the full paper in this repository (https://github.com/wi-re/spheric2024 or at this <a href="https://github.com/wi-re/spheric2024/blob/main/Physics_Based_ML___SPHERIC_2024.pdf">url</a></p> <p>You can also find the slides <a href="https://docs.google.com/presentation/d/1e02A_wJwa9tPF9eBA0_XZiz2FKnaNUiF/edit?usp=drive_link&amp;ouid=107302498783065501890&amp;rtpof=true&amp;sd=true">here</a></p> <h2 id="general-things">General things</h2> <p>Please also check out our recent work at ICLR Vienna 2024 https://tum-pbs.github.io/SFBC</p> <p>And our recent presentation at PMAC in Santa Fe https://pmac.fluids.dev</p> <h2 id="final-things">Final things</h2> <p>If you are interested in Machine Learning and SPH and especially if you are looking for a post-doc to work in this field, please contact me at rene.winchenbach@gmail.com</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[Physically-Motivated Machine Learning Models for Lagrangian Fluid Mechanics]]></summary></entry><entry><title type="html">partibench</title><link href="https://fluids.dev/blog/2024/party/" rel="alternate" type="text/html" title="partibench"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://fluids.dev/blog/2024/party</id><content type="html" xml:base="https://fluids.dev/blog/2024/party/"><![CDATA[<p>Particles!</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[partibench data and examples are now live]]></summary></entry><entry><title type="html">PMAC 2024 Workshop</title><link href="https://fluids.dev/blog/2024/pmac/" rel="alternate" type="text/html" title="PMAC 2024 Workshop"/><published>2024-01-01T00:32:13+00:00</published><updated>2024-01-01T00:32:13+00:00</updated><id>https://fluids.dev/blog/2024/pmac</id><content type="html" xml:base="https://fluids.dev/blog/2024/pmac/"><![CDATA[<h1 id="cross-validation-of-sph-based-machine-learning-models-using-the-taylor-green-vortex-case">Cross-Validation of SPH-based Machine Learning Models using the Taylor-Green Vortex Case</h1> <p>As presented at the <strong>Particle Methods and Applications Conference 2024</strong> co-hosted by LANL and SPHERIC.</p> <p>The Slides: https://docs.google.com/presentation/d/1W1yhR9dHK5ysy2fIDsSPK6_JHSIFXYCW/edit#slide=id.p1</p> <p>Our recent (May 2024) paper published at ICLR 2024: https://openreview.net/forum?id=HKgRwNhI9R</p> <p>Last Years live demo from SPHERIC 2023 (Rhodes) using Continuous Convolutions to learn a kernel function: https://colab.research.google.com/drive/1wdJoWvu1U5pftkjpjG0gb0ldSh5-CgDX (if the code doesnt work contact me please)</p> <p>If you have any suggestions or ideas or collaboration ideas contact me via email at rene.winchenbach at tum.de or find me on linked in: https://www.linkedin.com/in/rene-winchenbach-56a217257/</p> <h2 id="interesting-reads-if-you-are-interested-in-more-machine-learning-in-physics">Interesting reads if you are interested in more machine learning in physics</h2> <ul> <li>Learning Lagrangian Fluid Mechanics with Continuous Convolutions: the baseline of our current research https://openreview.net/forum?id=B1lDoJSYDH</li> <li>Guaranteed Conservation of Momentum for Learning Particle-based Fluid Dynamics: a follow up work on the prior paper that builds in hard constraints to ensure conservation of momentum https://arxiv.org/abs/2210.06036</li> <li>Physics informed machine learning with Smoothed Particle Hydrodynamics: Hierarchy of reduced Lagrangian models of turbulence: The paper presented by Michael Woodward at PMAC https://arxiv.org/abs/2110.13311</li> <li>Message-Passing Neural PDE Solvers: A state of the art graph based neural network approach to solving generic PDEs https://openreview.net/forum?id=vSix3HPYKSU</li> </ul> <h2 id="what-we-plan-on-doing-next">What we plan on doing next:</h2> <ul> <li>Hybrid solvers, e.g., low order SPH method + Neural network corrections to achieve higher order accuracy</li> <li>Investigating more general Radial Basis Function Networks for particle mechanics</li> <li>Learning specific subparts of an SPH simulation that are really expensive, e.g., the recent iterative particle shifting method</li> <li>Publishing our dataset (after some more work to make it more representative) as a dataset paper</li> </ul> <h2 id="finally-an-interesting-example-like-mentioned">Finally an interesting example like mentioned</h2> <p>Heres an example from a ground truth trajectory</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-05-22-13-14-57-480.webp 480w,/assets/img/2023-05-22-13-14-57-800.webp 800w,/assets/img/2023-05-22-13-14-57-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2023-05-22-13-14-57.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Shown here is a distribution of relative particle positions sampled over a few timesteps (left) and for a single step (right). In the ground truth we see a clear formation of two <em>rings</em> at $\Delta x$ and $2\Delta x$ with a minimum particle separation but an otherwise uniform distribution. Now consider a machine learning trajectory:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-05-22-13-18-55-480.webp 480w,/assets/img/2023-05-22-13-18-55-800.webp 800w,/assets/img/2023-05-22-13-18-55-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2023-05-22-13-18-55.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Here we still see the two rings and uniformity but the minimum particle separation is lower. This can often lead to issues during inference as those distributions are not included in the training data, albeit this can be reduced in impact by using unroll noise and data augmentation (to an extent). Now consider this method:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-05-22-13-19-31-480.webp 480w,/assets/img/2023-05-22-13-19-31-800.webp 800w,/assets/img/2023-05-22-13-19-31-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2023-05-22-13-19-31.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Which shows a highly biased distribution that aligns much closer to a rectilinear grid than an isotroptic distribution with a much lower minimum distance between particles. While this looks worse, and arguably is, this method is more stable in a lot of cases as this distribution acts as a stabilizing measure by regularizing the particle positions, similar to particle shifting. Note that the real reason we got these results was that the method in the last picture wasnt implemented properly and the <em>bug</em> lead to an interesting and, somewhat, improved result.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[Cross-Validation for Machine Learning]]></summary></entry><entry><title type="html">Sfbc</title><link href="https://fluids.dev/blog/2024/SFBC/" rel="alternate" type="text/html" title="Sfbc"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://fluids.dev/blog/2024/SFBC</id><content type="html" xml:base="https://fluids.dev/blog/2024/SFBC/"><![CDATA[<hr/> <p>layout: post title: SFBC (ICLR 2024) date: 2024-05-01 00:32:13 description: A collection of questions and information about our ICLR paper tags: formatting code categories: sample-posts</p> <h1 id="tabs-true">tabs: true</h1> <p>featured: true —</p> <h1 id="symmetric-fourier-basis-convolutions-for-learning-lagrangian-fluid-simulations">Symmetric Fourier Basis Convolutions for Learning Lagrangian Fluid Simulations</h1> <p>Authors: Rene Winchenbach, Nils Thuerey</p> <p>Accepted at: International Conference on Learning Representation (ICLR) 2024 - Vienna (as a Poster)</p> <p>If you have any questions on the paper itself or Continuous Convolutions in general, feel free to reach out to me via rene.winchenbach@tum.de</p> <p>Frequently asked questions about the paper will appear here as they are asked.</p> <p>Q: Can the method handle oversampled or undersampled data during inference related to training? A: No. The internal formulation of the Graph Convolution does not account for the volume of the contributing nodes, which works for our purposes as we are only dealing with uniformly sized particles and thus the volume is a constant term that can be ignored. However, if you wanted to do this you would need to change the Graph Convolution formulation to include volume.</p> <p>Q: What about scaling to other resolutions? A: Our method cannot handle this as there is some general uncertainty about particle resolutions as (a) scalar quantities in our SPH data scale with $h^{-d}$ and gradients with $h^{-d-1}$ and thus learning resolution varying quantities would require including this support much more tightly into the training. While this could potentially work, we did not investigate this for now.</p> <p>Q: What about adaptive resolutions? A: Similar to the prior question, this could conceptually be added to the network but makes the training significantly more complicated as there are many potential constellations of relative sizes and distributions a particle can see. This would make the dataset generation significantly harder and was beyond our scope.</p> <p>Q: What is the resolution to Fourier Neural Operators/Do you use FFTs? A: No. Our method works in normal coordinate space and not in a global frequency space. Instead our method works on a local limited convolution where we use the Fourier Terms to represent a filter function.</p> <p>Q: What about other appications? A: We did try our method on some general Pattern Recognitiion tasks but including these results was beyond our scope. Our Codebase can be applied to general graph tasks similar to how pyTorch geometric works.</p> <p>Q: What about larger simulations? A: Our method can readily do this due to its local Graph Convolution architecture. Accordingly, we can simply expand the simulation domain to be larger. Furthermore, the memory consumption during inference is relatively small so it would be possible to use multiple orders of magnitude more particles during inference if so desired.</p> <p>Q: How do you encode boundaries? A: Periodic boundaries are modeled by connecting particles across the periodic boundary with appropriate modular distances. Rigid boundaries are included with rigid particles with a seperate CConv on the first network layer.</p> <p>Repository: https://github.com/tum-pbs/SFBC ArXiV Paper: https://arxiv.org/abs/2403.16680</p>]]></content><author><name></name></author><summary type="html"><![CDATA[layout: post title: SFBC (ICLR 2024) date: 2024-05-01 00:32:13 description: A collection of questions and information about our ICLR paper tags: formatting code categories: sample-posts tabs: true featured: true —]]></summary></entry></feed>